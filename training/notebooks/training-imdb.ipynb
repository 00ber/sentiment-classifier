{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa76fd2-7277-4f52-a71b-33995bf86210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 10 06:15:20 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.98                 Driver Version: 535.98       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     Off | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 30%   22C    P8               3W / 250W |      0MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756adc3a-0bfd-40df-ae97-ca9344675d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID to sentiment: {0: 'negative', 1: 'positive'}\n",
      "Sentiment to ID: {'negative': 0, 'positive': 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"../data/imdb-dataset.csv\", split=\"train\")\n",
    "\n",
    "# Set sentiment <---> class id mappings\n",
    "sentiments = ['positive', \"negative\"]\n",
    "ID2SENT = { idx: sentiment for idx, sentiment in enumerate(sorted(sentiments)) }\n",
    "SENT2ID = { sentiment: idx for idx, sentiment in enumerate(sorted(sentiments)) }\n",
    "NUM_LABELS = len(sentiments)\n",
    "print(f\"ID to sentiment: {ID2SENT}\")\n",
    "print(f\"Sentiment to ID: {SENT2ID}\")\n",
    "\n",
    "##########################################\n",
    "# Change labels to class ids\n",
    "LABEL_COLUMN = \"labels\"\n",
    "TEXT_COLUMN = \"text\"\n",
    "dataset = dataset.class_encode_column(\"sentiment\")\n",
    "dataset = dataset.align_labels_with_mapping(SENT2ID, \"sentiment\")\n",
    "dataset = (dataset\n",
    "    .rename_column(\"review\", TEXT_COLUMN)\n",
    "    .rename_column(\"sentiment\", LABEL_COLUMN)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca83cb8-9a1e-4508-b28a-34d1fe6d7230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 40050\n",
      "Validation dataset size: 4950\n",
      "Test dataset size: 5000\n"
     ]
    }
   ],
   "source": [
    "# Generate train, val, test split\n",
    "split = dataset.train_test_split(test_size=0.1, stratify_by_column=LABEL_COLUMN)\n",
    "train_val = split[\"train\"].train_test_split(test_size=0.11, stratify_by_column=LABEL_COLUMN)\n",
    "train_dataset = train_val[\"train\"]\n",
    "val_dataset = train_val[\"test\"]\n",
    "test_dataset = split[\"test\"]\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50355fde-2470-4a5a-bbab-d61e223d4a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,042 || all params: 125,534,212 || trainable%: 0.7066137476531099\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "base_model = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, add_prefix_space=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model, \n",
    "    num_labels=NUM_LABELS,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, \n",
    "    r=8, \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef08f4b-511e-4e0c-ab23-a3fade5825fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718c6cddcab44eb092486f34d9fe0f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40050 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47757e5549704ae08f3988bb68428c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6aa6251331842d1944b539e112da89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize and prepare dataset\n",
    "CUTOFF_LENGTH = 256\n",
    "\n",
    "def tokenize_dataset(examples):\n",
    "    return tokenizer(examples[TEXT_COLUMN], truncation=True, max_length=CUTOFF_LENGTH)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_dataset, batched=True, remove_columns=TEXT_COLUMN)\n",
    "val_dataset = val_dataset.map(tokenize_dataset, batched=True, remove_columns=TEXT_COLUMN)\n",
    "test_dataset = test_dataset.map(tokenize_dataset, batched=True, remove_columns=TEXT_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb040af1-0de8-4d8c-89dd-4eb31020a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "train_dataset.set_format(\"torch\")\n",
    "val_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4cb05c6-11ed-481c-820b-c7842f0588d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    precision_score = precision.compute(predictions=predictions, references=labels, average=\"macro\", zero_division=1)[\"precision\"]\n",
    "    recall_score = recall.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score,\n",
    "        \"precision\": precision_score,\n",
    "        \"recall\": recall_score,\n",
    "        \"f1\": f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f36d3b58-8506-4c4c-b259-6966436865b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "lr = 5e-4\n",
    "batch_size = 128\n",
    "num_epochs = 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/lora\",\n",
    "    learning_rate=lr,\n",
    "    lr_scheduler_type= \"cosine_with_restarts\", #https://huggingface.co/transformers/v4.7.0/_modules/transformers/trainer_utils.html#:~:text=class-,SchedulerType,-(ExplicitEnum)%3A\n",
    "    warmup_ratio= 0.1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    # weight_decay=0.001,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=200,\n",
    "    eval_steps=200,\n",
    "    save_steps=400,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"none\",\n",
    "    fp16=False,\n",
    "    save_total_limit=1,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\":False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a238c769-9ef7-4a1f-a4ca-6246d9bf50b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1252' max='1252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1252/1252 48:50, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.212768</td>\n",
       "      <td>0.916162</td>\n",
       "      <td>0.916467</td>\n",
       "      <td>0.916162</td>\n",
       "      <td>0.916146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>0.205232</td>\n",
       "      <td>0.923434</td>\n",
       "      <td>0.923529</td>\n",
       "      <td>0.923434</td>\n",
       "      <td>0.923430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.189300</td>\n",
       "      <td>0.186536</td>\n",
       "      <td>0.922424</td>\n",
       "      <td>0.922514</td>\n",
       "      <td>0.922424</td>\n",
       "      <td>0.922420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.192401</td>\n",
       "      <td>0.927879</td>\n",
       "      <td>0.927930</td>\n",
       "      <td>0.927879</td>\n",
       "      <td>0.927877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.158300</td>\n",
       "      <td>0.190073</td>\n",
       "      <td>0.924848</td>\n",
       "      <td>0.925338</td>\n",
       "      <td>0.924848</td>\n",
       "      <td>0.924827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.188929</td>\n",
       "      <td>0.927071</td>\n",
       "      <td>0.927330</td>\n",
       "      <td>0.927071</td>\n",
       "      <td>0.927060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1252, training_loss=0.1952643638220839, metrics={'train_runtime': 2934.7055, 'train_samples_per_second': 54.588, 'train_steps_per_second': 0.427, 'total_flos': 2.12934674755584e+16, 'train_loss': 0.1952643638220839, 'epoch': 4.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1af0ed25-e546-47a3-9c2e-b7ccd4cc7df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2d33faab794549932adf8295db8889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/3.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62a7caab46c43f3aab768ae88a8df41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.push_to_hub(\"00BER/imbd-roberta-base-sentiment-lora-latest\")\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.push_to_hub(\"00BER/imbd-roberta-base-sentiment-merged-latest\")\n",
    "merged_model.save_pretrained(\"../models/imbd-roberta-base-sentiment-merged-latest\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
